# Turbine-LLM

Light weight inference optimized models for populare LLMs.

This sub-project is a work in progress. It is intended to be a repository of
layers, model recipes, and conversion tools from popular LLM quantization
tooling.

Models compiled from this repository can be served viw the adjacent
Turbine-Serving framework.
